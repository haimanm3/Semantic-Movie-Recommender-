{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebde4562-6320-45d4-a541-1f3148dee605",
   "metadata": {},
   "source": [
    "# Movie Recommendation System using TF-IDF and Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6607f-ea2d-4ae3-9e71-dd8157926425",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "First, let's import all necessary libraries and download required NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58fd13c1-9076-45f5-a7af-5e4efb40ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\haima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\haima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd709aa-966a-46a7-9152-0ffc940e09ae",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "Load the IMDB movies dataset and create preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60a19ff5-88aa-4e38-a910-cc91ee1066ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing movie plots...\n",
      "\n",
      "Preprocessing Results Example:\n",
      "                      title  \\\n",
      "0  The Shawshank Redemption   \n",
      "1             The Godfather   \n",
      "2     The Godfather Part II   \n",
      "3          Schindler's List   \n",
      "4              12 Angry Men   \n",
      "\n",
      "                                            overview  \\\n",
      "0  Imprisoned in the 1940s for the double murder ...   \n",
      "1  Spanning the years 1945 to 1955, a chronicle o...   \n",
      "2  In the continuing saga of the Corleone crime f...   \n",
      "3  The true story of how businessman Oskar Schind...   \n",
      "4  The defense and the prosecution have rested an...   \n",
      "\n",
      "                                      processed_plot  \n",
      "0  imprisoned 1940s double murder wife lover upst...  \n",
      "1  spanning year 1945 1955 chronicle fictional it...  \n",
      "2  continuing saga corleone crime family young vi...  \n",
      "3  true story businessman oskar schindler saved t...  \n",
      "4  defense prosecution rested jury filing jury ro...  \n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading and Preprocessing\n",
    "# Load the dataset\n",
    "df = pd.read_csv('imdb_movies.csv')\n",
    "\n",
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text following requirements:\n",
    "    - Lowercase\n",
    "    - Remove punctuation\n",
    "    - Remove stopwords\n",
    "    - Lemmatize\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "        # Tokenize\n",
    "        tokens = text.split()\n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "                 if token not in stop_words and token.isalnum()]\n",
    "        return ' '.join(tokens)\n",
    "    return ''\n",
    "\n",
    "# Preprocess all movie plots\n",
    "print(\"Preprocessing movie plots...\")\n",
    "df['processed_plot'] = df['overview'].fillna('').apply(preprocess_text)\n",
    "\n",
    "# Display preprocessing results\n",
    "print(\"\\nPreprocessing Results Example:\")\n",
    "print(df[['title', 'overview', 'processed_plot']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678c9f1-2d80-4045-909a-af469f1b1258",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization and Similarity Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c4b451a-e223-450a-82a5-151c67542a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TF-IDF vectorizer...\n",
      "\n",
      "Query: 'dreams and memory invasion'\n",
      "--------------------------------------------------\n",
      "1. The Trust (2016) - Score: 0.2600\n",
      "2. Eternal Sunshine of the Spotless Mind (2004) - Score: 0.2536\n",
      "3. Total Recall (1990) - Score: 0.2474\n",
      "4. Infinite (2021) - Score: 0.2339\n",
      "5. Ben 10: Race Against Time (2008) - Score: 0.2235\n",
      "\n",
      "Query: 'space exploration and aliens'\n",
      "--------------------------------------------------\n",
      "1. Space Chimps (2008) - Score: 0.3654\n",
      "2. Alien: Romulus (2024) - Score: 0.2789\n",
      "3. Lifeforce (1985) - Score: 0.2709\n",
      "4. Space Pirate Captain Harlock (2013) - Score: 0.2672\n",
      "5. Like Crazy (2016) - Score: 0.2583\n",
      "\n",
      "Query: 'superhero action adventure'\n",
      "--------------------------------------------------\n",
      "1. Shazam! (2019) - Score: 0.2743\n",
      "2. Max Steel (2016) - Score: 0.2727\n",
      "3. Minnal Murali (2021) - Score: 0.2576\n",
      "4. The Legend of Zorro (2005) - Score: 0.2263\n",
      "5. X-Men (2000) - Score: 0.2182\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Implementation\n",
    "# Create and fit TF-IDF vectorizer\n",
    "print(\"Training TF-IDF vectorizer...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_plot'])\n",
    "\n",
    "def get_tfidf_recommendations(input_text, top_n=5):\n",
    "    \"\"\"Get movie recommendations using TF-IDF and cosine similarity.\"\"\"\n",
    "    # Preprocess input text\n",
    "    processed_input = preprocess_text(input_text)\n",
    "    \n",
    "    # Transform input text using TF-IDF vectorizer\n",
    "    input_vector = tfidf_vectorizer.transform([processed_input])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(input_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get top N similar movies\n",
    "    movie_similarities = list(enumerate(similarities))\n",
    "    movie_similarities = sorted(movie_similarities, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = []\n",
    "    seen_titles = set()\n",
    "    \n",
    "    for idx, score in movie_similarities:\n",
    "        title = df.iloc[idx]['title']\n",
    "        if title not in seen_titles:\n",
    "            recommendations.append({\n",
    "                'Title': title,\n",
    "                'Year': df.iloc[idx]['release_date'][:4],\n",
    "                'Similarity Score': score\n",
    "            })\n",
    "            seen_titles.add(title)\n",
    "            \n",
    "            if len(recommendations) == top_n:\n",
    "                break\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test with multiple queries\n",
    "test_queries = [\n",
    "    \"dreams and memory invasion\",\n",
    "    \"space exploration and aliens\",\n",
    "    \"superhero action adventure\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    recommendations = get_tfidf_recommendations(query)\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec['Title']} ({rec['Year']}) - Score: {rec['Similarity Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee377b-8be1-4fc4-b9b1-bce105fffe44",
   "metadata": {},
   "source": [
    "## Part 2: Word2Vec Based Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25ff706d-c734-4b1e-a4ec-b2d0f634caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "\n",
      "Testing Word2Vec Recommendations:\n",
      "\n",
      "Query: 'dreams and memory invasion'\n",
      "--------------------------------------------------\n",
      "1. Thank You for Your Service (2017) - Score: 0.7043\n",
      "2. The Best Years (2020) - Score: 0.7035\n",
      "3. Universal Soldier (1992) - Score: 0.7021\n",
      "4. Total Recall (1990) - Score: 0.7001\n",
      "5. Paris or Perish (2013) - Score: 0.6979\n",
      "\n",
      "Query: 'space exploration and aliens'\n",
      "--------------------------------------------------\n",
      "1. Space Chimps (2008) - Score: 0.8185\n",
      "2. Meet Dave (2008) - Score: 0.8034\n",
      "3. Dead Space: Downfall (2008) - Score: 0.7874\n",
      "4. Lost in Space (1998) - Score: 0.7779\n",
      "5. Lifeforce (1985) - Score: 0.7771\n",
      "\n",
      "Query: 'superhero action adventure'\n",
      "--------------------------------------------------\n",
      "1. Max Steel (2016) - Score: 0.6936\n",
      "2. My Little Pony: The Movie (2017) - Score: 0.6676\n",
      "3. Aladdin (2019) - Score: 0.6607\n",
      "4. The Scorpion King 3: Battle for Redemption (2012) - Score: 0.6567\n",
      "5. PAW Patrol: Mighty Pups (2018) - Score: 0.6520\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec Implementation\n",
    "print(\"Training Word2Vec model...\")\n",
    "\n",
    "# Prepare sentences for Word2Vec\n",
    "sentences = [text.split() for text in df['processed_plot']]\n",
    "\n",
    "# Train Word2Vec model with final optimized parameters\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences, \n",
    "    vector_size=100,     # Reduced to focus on core semantics\n",
    "    window=5,            # Standard window size\n",
    "    min_count=2,         # Remove very rare words\n",
    "    workers=4,\n",
    "    sg=1,               # Skip-gram model\n",
    "    epochs=20           # More training epochs\n",
    ")\n",
    "\n",
    "# Calculate average vectors for each movie\n",
    "movie_vectors = []\n",
    "for text in df['processed_plot']:\n",
    "    words = text.split()\n",
    "    # Get vectors for meaningful words\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words \n",
    "                   if word in word2vec_model.wv and len(word) > 2]\n",
    "    \n",
    "    if word_vectors:\n",
    "        # Use weighted average based on word importance\n",
    "        movie_vector = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        movie_vector = np.zeros(100)\n",
    "    movie_vectors.append(movie_vector)\n",
    "movie_vectors = np.array(movie_vectors)\n",
    "\n",
    "def get_word2vec_recommendations(input_text, top_n=5):\n",
    "    \"\"\"Get movie recommendations using Word2Vec and cosine similarity.\"\"\"\n",
    "    # Preprocess input text\n",
    "    processed_input = preprocess_text(input_text)\n",
    "    \n",
    "    # Calculate input vector\n",
    "    input_words = processed_input.split()\n",
    "    input_vectors = [word2vec_model.wv[word] for word in input_words \n",
    "                    if word in word2vec_model.wv and len(word) > 2]\n",
    "    \n",
    "    if not input_vectors:\n",
    "        return []\n",
    "    \n",
    "    input_vector = np.mean(input_vectors, axis=0).reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(input_vector, movie_vectors).flatten()\n",
    "    \n",
    "    # Get top N similar movies\n",
    "    movie_similarities = list(enumerate(similarities))\n",
    "    movie_similarities = sorted(movie_similarities, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = []\n",
    "    seen_titles = set()\n",
    "    \n",
    "    for idx, score in movie_similarities:\n",
    "        title = df.iloc[idx]['title']\n",
    "        if title not in seen_titles and score > 0.3:  # Lower threshold for more diversity\n",
    "            recommendations.append({\n",
    "                'Title': title,\n",
    "                'Year': df.iloc[idx]['release_date'][:4],\n",
    "                'Similarity Score': score\n",
    "            })\n",
    "            seen_titles.add(title)\n",
    "            \n",
    "            if len(recommendations) == top_n:\n",
    "                break\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test Word2Vec recommendations\n",
    "print(\"\\nTesting Word2Vec Recommendations:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    recommendations = get_word2vec_recommendations(query)\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec['Title']} ({rec['Year']}) - Score: {rec['Similarity Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d574b-ee5f-478b-9d49-71d717b05143",
   "metadata": {},
   "source": [
    "## Part 3: Comparison and Analysis of Both Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cdbb7d50-a39f-4ba9-9f0e-9f1720c399c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'dreams and memory invasion'\n",
      "\n",
      "TF-IDF Recommendations:\n",
      "--------------------------------------------------\n",
      "1. The Trust (2016) - Score: 0.2600\n",
      "2. Eternal Sunshine of the Spotless Mind (2004) - Score: 0.2536\n",
      "3. Total Recall (1990) - Score: 0.2474\n",
      "4. Infinite (2021) - Score: 0.2339\n",
      "5. Ben 10: Race Against Time (2008) - Score: 0.2235\n",
      "\n",
      "Word2Vec Recommendations:\n",
      "--------------------------------------------------\n",
      "1. Thank You for Your Service (2017) - Score: 0.7043\n",
      "2. The Best Years (2020) - Score: 0.7035\n",
      "3. Universal Soldier (1992) - Score: 0.7021\n",
      "4. Total Recall (1990) - Score: 0.7001\n",
      "5. Paris or Perish (2013) - Score: 0.6979\n"
     ]
    }
   ],
   "source": [
    "# Direct Comparison Function\n",
    "def compare_methods(query):\n",
    "    \"\"\"Compare recommendations from both TF-IDF and Word2Vec methods.\"\"\"\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    \n",
    "    print(\"\\nTF-IDF Recommendations:\")\n",
    "    print(\"-\" * 50)\n",
    "    tfidf_recs = get_tfidf_recommendations(query)\n",
    "    for i, rec in enumerate(tfidf_recs, 1):\n",
    "        print(f\"{i}. {rec['Title']} ({rec['Year']}) - Score: {rec['Similarity Score']:.4f}\")\n",
    "    \n",
    "    print(\"\\nWord2Vec Recommendations:\")\n",
    "    print(\"-\" * 50)\n",
    "    w2v_recs = get_word2vec_recommendations(query)\n",
    "    for i, rec in enumerate(w2v_recs, 1):\n",
    "        print(f\"{i}. {rec['Title']} ({rec['Year']}) - Score: {rec['Similarity Score']:.4f}\")\n",
    "\n",
    "# Test both methods with a sample query\n",
    "test_query = \"dreams and memory invasion\"\n",
    "compare_methods(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461da03-e4e5-4b01-8cb9-76a9b5121d5b",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "### Comparison of TF-IDF vs Word2Vec Results\n",
    "\n",
    "#### TF-IDF Performance\n",
    "- Shows more consistent thematic matching\n",
    "- Better at handling specific plot elements\n",
    "- Example: Found \"Eternal Sunshine\" and \"Total Recall\" for memory-related query\n",
    "- Similarity scores range from ~0.20 to 0.35\n",
    "- More reliable for direct keyword matching\n",
    "\n",
    "#### Word2Vec Performance\n",
    "- Shows broader semantic relationships\n",
    "- Captures some thematic similarities\n",
    "- Example: Found \"Universal Soldier\" and \"Total Recall\" for memory query\n",
    "- Similarity scores tend to be higher (0.65-0.82)\n",
    "- Better at finding related concepts even with different vocabulary\n",
    "\n",
    "### Key Differences Observed\n",
    "\n",
    "#### 1. Similarity Scores\n",
    "- TF-IDF: More conservative scores (0.20-0.35)\n",
    "- Word2Vec: Higher scores overall (0.65-0.82)\n",
    "- TF-IDF scores seem more reliable for ranking\n",
    "\n",
    "#### 2. Types of Matches\n",
    "- TF-IDF: More literal matches based on shared words\n",
    "- Word2Vec: More conceptual matches based on word relationships\n",
    "- Example: Space query\n",
    "  * TF-IDF found direct matches like \"Space Chimps\"\n",
    "  * Word2Vec found related concepts like \"Dead Space\" and \"Lost in Space\"\n",
    "\n",
    "#### 3. Consistency\n",
    "- TF-IDF showed more consistent genre-appropriate matches\n",
    "- Word2Vec sometimes included thematically distant movies\n",
    "- TF-IDF better maintained genre boundaries\n",
    "\n",
    "### Why the Differences?\n",
    "\n",
    "#### TF-IDF Strengths\n",
    "- Works well with specific vocabulary\n",
    "- Good for plot-based similarity\n",
    "- More predictable results\n",
    "- Better for direct content matching\n",
    "\n",
    "#### Word2Vec Strengths\n",
    "- Captures word relationships\n",
    "- Can find similar concepts\n",
    "- More flexible with vocabulary\n",
    "- Better for thematic similarity\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "For this movie recommendation task:\n",
    "1. TF-IDF appears more reliable for plot-based recommendations\n",
    "2. Word2Vec shows promise but needs refinement\n",
    "3. TF-IDF's more conservative approach yields more relevant results\n",
    "4. Both methods successfully avoid duplicates\n",
    "5. A hybrid approach might be worth exploring in future iterations\n",
    "\n",
    "The choice between methods depends on the use case:\n",
    "- Use TF-IDF for specific plot-based searches\n",
    "- Use Word2Vec for broader thematic searches\n",
    "- Consider combining both for a more robust system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
